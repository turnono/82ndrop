"""
82ndrop Agent System Evaluations

Tests the complete agent workflow including:
- TaskMaster coordination
- Guide Agent schema creation
- Search Agent enrichment
- PromptWriter final output generation
- JSON format validation
- Creative prompt handling
"""

import asyncio
import logging
import time
import json
import sys
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

# Add project root to path for imports
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(name)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


async def test_agent_configuration():
    """Test that the agent system is properly configured"""
    
    print("üîÑ Testing Agent Configuration")
    
    start_time = time.time()
    
    try:
        # Test agent imports and structure
        from drop_agent.agent import root_agent
        from drop_agent.sub_agents.guide.agent import guide_agent
        from drop_agent.sub_agents.search.agent import search_agent
        from drop_agent.sub_agents.prompt_writer.agent import prompt_writer_agent
        
        # Verify root agent configuration
        if root_agent.name != "task_master_agent":
            raise Exception(f"Root agent name incorrect: {root_agent.name}")
            
        if root_agent.model != "gemini-2.0-flash":
            raise Exception(f"Root agent model incorrect: {root_agent.model}")
        
        # Verify sub-agents are configured
        if len(root_agent.sub_agents) != 3:
            raise Exception(f"Expected 3 sub-agents, got {len(root_agent.sub_agents)}")
            
        # Verify sub-agents exist and have correct names
        expected_agents = ["guide_agent", "search_agent", "prompt_writer_agent"]
        actual_agents = [agent.name for agent in root_agent.sub_agents]
        
        for expected in expected_agents:
            if expected not in actual_agents:
                raise Exception(f"Missing agent: {expected}")
        
        # Verify each sub-agent has the correct model
        for agent in root_agent.sub_agents:
            if agent.model != "gemini-2.0-flash":
                raise Exception(f"Agent {agent.name} has wrong model: {agent.model}")
                
        duration = time.time() - start_time
        print(f"‚úÖ PASS: Agent configuration test completed in {duration:.2f}s")
        
        return {
            "test_name": "agent_configuration",
            "passed": True,
            "duration": duration,
            "metrics": {
                "root_agent_name": root_agent.name,
                "root_agent_model": root_agent.model,
                "sub_agents_count": len(root_agent.sub_agents),
                "sub_agent_names": actual_agents,
            },
        }
        
    except Exception as e:
        duration = time.time() - start_time
        print(f"‚ùå FAIL: Agent configuration test failed: {e}")
        return {
            "test_name": "agent_configuration",
            "passed": False,
            "error": str(e),
            "duration": duration,
        }


async def test_prompt_structures():
    """Test that prompts are properly structured"""
    
    print("üìù Testing Prompt Structures")
    
    start_time = time.time()
    
    try:
        from drop_agent.prompts import PROMPT
        from drop_agent.sub_agents.guide.prompt import INSTRUCTION as GUIDE_INSTRUCTION
        from drop_agent.sub_agents.search.prompt import INSTRUCTION as SEARCH_INSTRUCTION
        from drop_agent.sub_agents.prompt_writer.prompt import INSTRUCTION as PROMPT_WRITER_INSTRUCTION
        
        # Verify prompts exist and are non-empty
        prompts = {
            "root_prompt": PROMPT,
            "guide_instruction": GUIDE_INSTRUCTION,
            "search_instruction": SEARCH_INSTRUCTION,
            "prompt_writer_instruction": PROMPT_WRITER_INSTRUCTION,
        }
        
        for name, prompt in prompts.items():
            if not prompt or len(prompt.strip()) < 50:
                raise Exception(f"{name} is too short or empty")
        
        # Verify root prompt mentions coordination or team work
        coordination_keywords = ["coordinate", "orchestrate", "team", "specialist", "sub-agent"]
        if not any(keyword in PROMPT.lower() for keyword in coordination_keywords):
            raise Exception("Root prompt doesn't mention coordination/orchestration or team work")
            
        # Verify prompt writer mentions JSON
        if "json" not in PROMPT_WRITER_INSTRUCTION.lower():
            raise Exception("Prompt writer doesn't mention JSON output")
            
        duration = time.time() - start_time
        print(f"‚úÖ PASS: Prompt structures test completed in {duration:.2f}s")
        
        return {
            "test_name": "prompt_structures",
            "passed": True,
            "duration": duration,
            "metrics": {
                "prompts_checked": len(prompts),
                "avg_prompt_length": sum(len(p) for p in prompts.values()) // len(prompts),
            },
        }
        
    except Exception as e:
        duration = time.time() - start_time
        print(f"‚ùå FAIL: Prompt structures test failed: {e}")
        return {
            "test_name": "prompt_structures",
            "passed": False,
            "error": str(e),
            "duration": duration,
        }


async def test_json_format_validation():
    """Test JSON format validation logic"""
    
    print("üîç Testing JSON Format Validation")
    
    start_time = time.time()
    
    try:
        # Test valid JSON examples from terminal session
        valid_examples = [
            '```json\n{\n  "top": "Wall Street\'s Newest Analyst",\n  "center": "A gorilla in a suit passionately explains stock market trends using charts and graphs.",\n  "bottom": "Invest like a beast! #stocks #finance #gorilla"\n}\n```',
            '```json\n{\n  "top": "Overheard in the Animal Kingdom",\n  "center": "Animals hilariously commentating on human behavior",\n  "bottom": "They think WE\'RE the weird ones..."\n}\n```',
        ]
        
        for i, example in enumerate(valid_examples):
            # Extract JSON
            json_start = example.find("```json") + 7
            json_end = example.find("```", json_start)
            json_content = example[json_start:json_end].strip()
            
            # Parse JSON
            parsed = json.loads(json_content)
            
            # Validate required fields
            required_fields = ["top", "center", "bottom"]
            for field in required_fields:
                if field not in parsed:
                    raise Exception(f"Example {i+1} missing field: {field}")
                    
            # Validate field types and content
            for field in required_fields:
                if not isinstance(parsed[field], str) or len(parsed[field].strip()) == 0:
                    raise Exception(f"Example {i+1} field {field} is not a valid string")
        
        duration = time.time() - start_time
        print(f"‚úÖ PASS: JSON format validation test completed in {duration:.2f}s")
        
        return {
            "test_name": "json_format_validation",
            "passed": True,
            "duration": duration,
            "metrics": {
                "examples_tested": len(valid_examples),
                "fields_per_example": len(required_fields),
            },
        }
        
    except Exception as e:
        duration = time.time() - start_time
        print(f"‚ùå FAIL: JSON format validation test failed: {e}")
        return {
            "test_name": "json_format_validation",
            "passed": False,
            "error": str(e),
            "duration": duration,
        }


async def test_environment_setup():
    """Test that the environment is properly configured"""
    
    print("üåç Testing Environment Setup")
    
    start_time = time.time()
    
    try:
        import os
        load_dotenv()
        
        # Check for required environment variables
        required_vars = ["GOOGLE_API_KEY"]
        missing_vars = []
        
        for var in required_vars:
            if not os.getenv(var):
                missing_vars.append(var)
        
        if missing_vars:
            print(f"‚ö†Ô∏è  Missing environment variables: {missing_vars}")
            print("This might affect live testing but structure tests can still pass")
        
        # Test imports work
        try:
            from google.adk import Agent
            from google.adk.tools.agent_tool import AgentTool
            import google.genai
        except ImportError as e:
            raise Exception(f"Required packages not available: {e}")
            
        duration = time.time() - start_time
        print(f"‚úÖ PASS: Environment setup test completed in {duration:.2f}s")
        
        return {
            "test_name": "environment_setup",
            "passed": True,
            "duration": duration,
            "metrics": {
                "missing_env_vars": len(missing_vars),
                "required_packages_available": True,
            },
        }
        
    except Exception as e:
        duration = time.time() - start_time
        print(f"‚ùå FAIL: Environment setup test failed: {e}")
        return {
            "test_name": "environment_setup",
            "passed": False,
            "error": str(e),
            "duration": duration,
        }


async def test_successful_examples_analysis():
    """Analyze the successful examples from terminal session"""
    
    print("üìä Testing Successful Examples Analysis")
    
    start_time = time.time()
    
    try:
        # Examples from the successful terminal session
        successful_examples = [
            {
                "input": "A gorilla in a business suit explaining the stock market in a cinematic style.",
                "output": {
                    "top": "Wall Street's Newest Analyst",
                    "center": "A gorilla in a suit passionately explains stock market trends using charts and graphs.",
                    "bottom": "Invest like a beast! #stocks #finance #gorilla"
                }
            },
            {
                "input": "Make a funny video about what animals think about humans.",
                "output": {
                    "top": "Overheard in the Animal Kingdom",
                    "center": "Animals hilariously commentating on human behavior",
                    "bottom": "They think WE'RE the weird ones..."
                }
            },
            {
                "input": "Gorilla podcast, but they're reviewing the latest action movie.",
                "output": {
                    "top": "Ape About Movies Podcast",
                    "center": "Gorilla reviews 'Rampage' (2018)",
                    "bottom": "Rate üçåüçåüçå if you agree!"
                }
            },
            {
                "input": "Let's do a movie remake of The Matrix, but make Neo a confused golden retriever.",
                "output": {
                    "top": "What if The Matrix was wholesome?",
                    "center": "Golden Retriever realizes he's living in a simulation and uses his powers for good",
                    "bottom": "He just wants belly rubs and world peace"
                }
            }
        ]
        
        # Analyze the patterns
        metrics = {
            "total_examples": len(successful_examples),
            "avg_input_length": sum(len(ex["input"]) for ex in successful_examples) // len(successful_examples),
            "avg_top_length": sum(len(ex["output"]["top"]) for ex in successful_examples) // len(successful_examples),
            "avg_center_length": sum(len(ex["output"]["center"]) for ex in successful_examples) // len(successful_examples),
            "avg_bottom_length": sum(len(ex["output"]["bottom"]) for ex in successful_examples) // len(successful_examples),
        }
        
        # Validate each example maintains key themes
        for i, example in enumerate(successful_examples):
            output = example["output"]
            
            # Check that outputs are engaging and creative
            if len(output["top"]) < 10:
                raise Exception(f"Example {i+1} top section too short")
                
            if len(output["center"]) < 20:
                raise Exception(f"Example {i+1} center section too short")
                
            if len(output["bottom"]) < 10:
                raise Exception(f"Example {i+1} bottom section too short")
        
        duration = time.time() - start_time
        print(f"‚úÖ PASS: Successful examples analysis completed in {duration:.2f}s")
        print(f"   Analyzed {metrics['total_examples']} successful examples")
        
        return {
            "test_name": "successful_examples_analysis",
            "passed": True,
            "duration": duration,
            "metrics": metrics,
        }
        
    except Exception as e:
        duration = time.time() - start_time
        print(f"‚ùå FAIL: Successful examples analysis test failed: {e}")
        return {
            "test_name": "successful_examples_analysis",
            "passed": False,
            "error": str(e),
            "duration": duration,
        }


async def run_82ndrop_evaluations():
    """Run all 82ndrop agent evaluation tests"""
    
    print("üî¨ Starting 82ndrop Agent System Evaluations")
    print("=" * 60)
    
    tests = [
        test_environment_setup,
        test_agent_configuration,
        test_prompt_structures,
        test_json_format_validation,
        test_successful_examples_analysis,
    ]
    
    results = []
    start_time = datetime.now()
    
    for test_func in tests:
        try:
            result = await test_func()
            results.append(result)
        except Exception as e:
            logger.error(f"Test {test_func.__name__} failed with error: {e}")
            results.append(
                {
                    "test_name": test_func.__name__,
                    "passed": False,
                    "error": str(e),
                    "duration": 0,
                }
            )
        
        print()  # Add spacing between tests
    
    # Calculate summary
    end_time = datetime.now()
    total_duration = (end_time - start_time).total_seconds()
    passed_tests = sum(1 for r in results if r["passed"])
    total_tests = len(results)
    success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
    
    print("=" * 60)
    print("82ndrop Agent System Evaluation Results:")
    print(f"   Passed: {passed_tests}/{total_tests}")
    print(f"   Success Rate: {success_rate:.1f}%")
    print(f"   Total Duration: {total_duration:.2f}s")
    
    if success_rate == 100:
        print("üéâ All 82ndrop agent tests passed!")
        print("‚ú® System is ready for production use!")
    else:
        print("‚ö†Ô∏è  Some 82ndrop agent tests failed - check logs for details")
    
    # Add note about live testing
    print()
    print("üìù Note: These tests validate system structure and configuration.")
    print("   For live agent testing, use: adk run drop_agent")
    
    # Save detailed results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    Path("evals").mkdir(exist_ok=True)
    report_file = f"evals/82ndrop_eval_report_{timestamp}.json"
    
    detailed_report = {
        "summary": {
            "evaluation_info": {
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "total_duration": total_duration,
                "agent_system": "82ndrop",
                "test_environment": "development",
                "evaluation_type": "structural_validation",
            },
            "results": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": total_tests - passed_tests,
                "success_rate": success_rate,
            },
            "notes": [
                "These tests validate system structure and configuration",
                "Live agent testing available via: adk run drop_agent",
                "All successful examples from terminal session are documented",
            ]
        },
        "detailed_results": results,
    }
    
    with open(report_file, "w") as f:
        json.dump(detailed_report, f, indent=2, default=str)
    
    print(f"üìä Detailed report saved to: {report_file}")
    
    return detailed_report


if __name__ == "__main__":
    asyncio.run(run_82ndrop_evaluations()) 